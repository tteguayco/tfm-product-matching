{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Load</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_FILEPATH = \"../_data/MATCHING_DISTANCE_VECTORS.csv\"\n",
    "MODELS_EXPORT_PATH = \"../_models/\"\n",
    "\n",
    "df = pd.read_csv(DATASET_FILEPATH, header=0, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (375246, 10)\n",
      "y_train: (375246,)\n",
      "X_test: (160821, 10)\n",
      "y_test: (160821,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"MATCH\"])\n",
    "y = df[\"MATCH\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, shuffle=True)\n",
    "\n",
    "print(\"X_train: {}\".format(X_train.shape))\n",
    "print(\"y_train: {}\".format(y_train.shape))\n",
    "print(\"X_test: {}\".format(X_test.shape))\n",
    "print(\"y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Utils</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    cm_counts = cm\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "   \n",
    "    cmap=plt.cm.Blues\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, str(cm_counts[i, j]) + \" (\" + str(round(cm[i, j] * 100, 2)) + \"%)\",\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def display_model_evaluation(y_actual, y_pred, classes):\n",
    "    \n",
    "    if len(y_actual) != len(y_pred):\n",
    "        raise ValueError(\"Lengths of provided y_actual and y_pred do not match.\")\n",
    "    \n",
    "    cm = confusion_matrix(y_actual, y_pred)\n",
    "    acc_perc = round(accuracy_score(y_actual, y_pred) * 100, 2)\n",
    "    #recall = recall_score(y_actual, y_pred)\n",
    "    #prec = precision_score(y_actual, y_pred)\n",
    "    #f1 = f1_score(y_actual, y_pred)\n",
    "    \n",
    "    plot_confusion_matrix(cm, classes)\n",
    "    \n",
    "    print(\"Accuracy: {}%\".format(acc_perc))\n",
    "    #print(\"Recall: {}\".format(recall))\n",
    "    #print(\"Precision: {}\".format(prec))\n",
    "    #print(\"F1: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Persistance</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "def save_model_to_file(clf, filename):\n",
    "    dump(clf, MODELS_EXPORT_PATH + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Training</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K Nearest Neighbors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Support Vector Machines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "C = 450.07963080130486\n",
    "gamma = 9.711499749455284e-06\n",
    "\n",
    "svm_clf = svm.SVC(C=C, gamma=gamma, probability=True)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_model_evaluation(y_test, y_pred, set(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_to_file(svm_clf, \"svm_smartphones.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
